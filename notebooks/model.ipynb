{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('..')\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "import constants as cst\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    "\n",
    "from xgboost import XGBClassifier, plot_tree, plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(os.path.join('..', cst.FEATURES_PATH), index_col=0)\n",
    "training_target = pd.read_csv(os.path.join('..', cst.TRAIN_TARGET_PATH), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_training_data = pd.merge(training_data, training_target, on='client_id', how='left')\n",
    "full_training_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(full_training_data, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_set['is_churn']\n",
    "train_set.drop(columns=['is_churn'], inplace=True)\n",
    "\n",
    "y_test = test_set['is_churn']\n",
    "test_set.drop(columns=['is_churn'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_nume_cols = [\n",
    "    'mean_qty',\n",
    "    'mean_sales',\n",
    "    'n_branch',\n",
    "    'n_product',\n",
    "    'purchase_freq',\n",
    "    'n_purchases',\n",
    "    # 'client_age',\n",
    "    # 'time_from_last_purchase',\n",
    "    # 'client_lifetime',\n",
    "]\n",
    "unscaled_num_cols = [\n",
    "    'max_qty', \n",
    "    'min_qty', \n",
    "    'std_qty', \n",
    "    'last_qty_1',\n",
    "    'last_qty_2',\n",
    "    'last_qty_3',\n",
    "    'last_qty_4',\n",
    "    'max_sales',\n",
    "    'min_sales', \n",
    "    'std_sales', \n",
    "    'last_sales_1',\n",
    "    'last_sales_2',\n",
    "    'last_sales_3',\n",
    "    'last_sales_4', \n",
    "    'delay_purchase_n1',\n",
    "    'delay_purchase_n2',\n",
    "    'delay_purchase_n3',\n",
    "    'delay_purchase_n4',\n",
    "]\n",
    "drop_cols = ['client_id', 'frequency', 'client_category', 'time_from_last_purchase', 'client_lifetime', 'client_age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = make_column_transformer(\n",
    "    (StandardScaler(), scale_nume_cols),\n",
    "    ('passthrough', unscaled_num_cols),\n",
    "    ('drop', drop_cols)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(\n",
    "    transformer, XGBClassifier()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(train_set, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = pipeline.predict(train_set)\n",
    "test_pred = pipeline.predict(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy score: {accuracy_score(train_pred, y_train)}')\n",
    "print(f'Precision score: {precision_score(train_pred, y_train)}')\n",
    "print(f'Recall score: {recall_score(train_pred, y_train)}')\n",
    "print(f'F1 score: {f1_score(train_pred, y_train)}')\n",
    "print(f'{confusion_matrix(train_pred, y_train)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy score: {accuracy_score(test_pred, y_test)}')\n",
    "print(f'Precision score: {precision_score(test_pred, y_test)}')\n",
    "print(f'Recall score: {recall_score(test_pred, y_test)}')\n",
    "print(f'F1 score: {f1_score(test_pred, y_test)}')\n",
    "print(f'{confusion_matrix(test_pred, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.steps[1][1].get_booster().feature_names = [name.split('__')[1] for name in pipeline.steps[0][1].get_feature_names_out()]\n",
    "\n",
    "plot_importance(pipeline.steps[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load JS visualization code to notebook\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(pipeline.steps[1][1])\n",
    "shap_values = explainer.shap_values(pipeline.steps[0][1].fit_transform(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, features=pipeline.steps[0][1].fit_transform(train_set), \n",
    "                  feature_names=[name.split('__')[1] for name in pipeline.steps[0][1].get_feature_names_out()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.argwhere(train_pred==1)[0][0]\n",
    "shap.force_plot(\n",
    "    explainer.expected_value, \n",
    "    shap_values[i], \n",
    "    features=pipeline.steps[0][1].fit_transform(train_set)[i], \n",
    "    feature_names=[name.split('__')[1] for name in pipeline.steps[0][1].get_feature_names_out()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve non-churners\n",
    "val_transactions = pd.read_csv(os.path.join('..', cst.VALIDATION_DATA_PATH), index_col=0)\n",
    "non_churners = val_transactions['client_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether a client has churned in the validation period\n",
    "train_set.loc[train_set['client_id'].isin(non_churners), 'val_is_churn'] = 0\n",
    "train_set.loc[~(train_set['client_id'].isin(non_churners)), 'val_is_churn'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['val_is_churn'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy score: {accuracy_score(train_pred, train_set['val_is_churn'])}\")\n",
    "print(f\"Precision score: {precision_score(train_pred, train_set['val_is_churn'])}\")\n",
    "print(f\"Recall score: {recall_score(train_pred, train_set['val_is_churn'])}\")\n",
    "print(f\"F1 score: {f1_score(train_pred, train_set['val_is_churn'])}\")\n",
    "print(f\"{confusion_matrix(train_pred, train_set['val_is_churn'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_training_data['churn_prob'] = pipeline.predict_proba(full_training_data)[:, 1]\n",
    "full_training_data['churn_pred'] = pipeline.predict(full_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_training_data.loc[(full_training_data['client_id'].isin(non_churners)) & (full_training_data['churn_pred']==1), 'actionable'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_training_data.to_csv(os.path.join('..', cst.FULL_PREDICTIONS_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(pipeline.steps[1][1])\n",
    "shap_values = explainer.shap_values(pipeline.steps[0][1].fit_transform(full_training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join('..', cst.SHAP_VALUES_PATH), shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(os.path.join('..', cst.EXPLAINER_PATH), 'wb') as f:\n",
    "    pickle.dump(explainer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('..', cst.PIPELINE_PATH), 'wb') as f:\n",
    "    pickle.dump(pipeline, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_training_data[full_training_data['actionable']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "61c02cc6cc60e2aa9d27ab524b81e9f4bca9132d1b517bd5e0da16c7571c3623"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('bcg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
